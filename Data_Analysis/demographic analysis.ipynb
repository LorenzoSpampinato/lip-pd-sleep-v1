{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T11:14:09.135463Z",
     "start_time": "2025-03-10T11:14:06.380899Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "\n",
    "# ----------------------------\n",
    "# PARTE 1: CALCOLO PER LE VARIABILI NUMERICHE\n",
    "# ----------------------------\n",
    "\n",
    "# Percorso del file con i dati dei pazienti\n",
    "data_file = r\"C:\\Users\\Lorenzo\\Desktop\\scoring\\demographics/summary_extracted.xlsx\"\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "# Assicuriamoci che la colonna Group sia di tipo categoria\n",
    "df['Group'] = df['Group'].astype('category')\n",
    "\n",
    "# Lista delle variabili numeriche (continue e ordinali)\n",
    "numeric_vars = ['Age', 'Disease_Duration', 'LEDD', 'UPDRSIV4.1',\n",
    "                'UPDRSI', 'UPDRSII', 'UPDRSIII', 'UPDRSIV', 'AIMs']\n",
    "\n",
    "# Otteniamo i gruppi presenti\n",
    "groups = df['Group'].unique()\n",
    "\n",
    "results = []\n",
    "for var in numeric_vars:\n",
    "    result_row = {}\n",
    "    result_row['Variable'] = var\n",
    "\n",
    "    # --- Normalità (Test Shapiro-Wilk per ogni gruppo) ---\n",
    "    normality_pvalues = {}\n",
    "    decisions = []  # \"Normal\", \"Non-normal\", \"Insufficient\"\n",
    "    for grp in groups:\n",
    "        data_grp = df[df['Group'] == grp][var].dropna()\n",
    "        if len(data_grp) < 3:\n",
    "            normality_pvalues[grp] = \"n<3\"\n",
    "            decisions.append(\"Insufficient\")\n",
    "        else:\n",
    "            # Check for zero variance\n",
    "            if data_grp.var() == 0:\n",
    "                normality_pvalues[grp] = \"zero variance\"\n",
    "                decisions.append(\"Non-normal\")  # Zero variance means non-normal\n",
    "            else:\n",
    "                try:\n",
    "                    stat, p = stats.shapiro(data_grp)\n",
    "                    normality_pvalues[grp] = round(p, 3)\n",
    "                    decisions.append(\"Normal\" if p >= 0.05 else \"Non-normal\")\n",
    "                except Exception as e:\n",
    "                    normality_pvalues[grp] = f\"Error: {str(e)}\"\n",
    "                    decisions.append(\"Error\")\n",
    "                    \n",
    "    if all(dec == \"Insufficient\" for dec in decisions):\n",
    "        overall_normality = \"Insufficient data\"\n",
    "    elif any(dec == \"Non-normal\" for dec in decisions if dec != \"Insufficient\"):\n",
    "        overall_normality = \"Non-normal\"\n",
    "    else:\n",
    "        overall_normality = \"Normal\"\n",
    "    result_row['Normality'] = overall_normality\n",
    "    pvals_str = \"; \".join([f\"{grp}: {normality_pvalues[grp]}\" for grp in groups])\n",
    "    result_row['Shapiro p-values'] = pvals_str\n",
    "\n",
    "    # --- Omogeneità delle varianze (Test di Levene) ---\n",
    "    samples = []\n",
    "    valid_groups = []\n",
    "    for grp in groups:\n",
    "        data_grp = df[df['Group'] == grp][var].dropna()\n",
    "        if len(data_grp) > 1:\n",
    "            samples.append(data_grp.values)\n",
    "            valid_groups.append(grp)\n",
    "    if len(samples) < 2:\n",
    "        variance_status = \"Insufficient data\"\n",
    "        levene_p = \"NA\"\n",
    "    else:\n",
    "        try:\n",
    "            stat_levene, p_levene = stats.levene(*samples)\n",
    "            levene_p = round(p_levene, 3)\n",
    "            variance_status = \"Homogeneous\" if p_levene >= 0.05 else \"Heterogeneous\"\n",
    "        except Exception as e:\n",
    "            variance_status = f\"Error: {str(e)}\"\n",
    "            levene_p = \"Error\"\n",
    "    result_row['Variance Homogeneity'] = variance_status\n",
    "    result_row['Levene p-value'] = levene_p\n",
    "\n",
    "    results.append(result_row)\n",
    "\n",
    "numeric_summary_df = pd.DataFrame(results)\n",
    "numeric_summary_df = numeric_summary_df[['Variable', 'Shapiro p-values', 'Normality',\n",
    "                                         'Levene p-value', 'Variance Homogeneity']]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:14:43.028342Z",
     "start_time": "2025-03-10T11:14:42.962158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "desired_order = ['CTL', 'DNV', 'ADV', 'DYS']\n",
    "cat_results = []\n",
    "\n",
    "for grp in desired_order:\n",
    "    if grp not in df['Group'].unique():\n",
    "        continue\n",
    "    group_data = df[df['Group'] == grp]\n",
    "    total = len(group_data)\n",
    "    \n",
    "    # 1) Gender: nF / nM\n",
    "    if 'Gender' in df.columns:\n",
    "        n_F = sum(group_data['Gender'] == 'F')\n",
    "        n_M = sum(group_data['Gender'] == 'M')\n",
    "        gender_summary = f\"{n_F} F / {n_M} M\"\n",
    "        cat_results.append({\n",
    "            \"Group\": grp,\n",
    "            \"Variable\": \"Gender\",\n",
    "            \"Summary\": gender_summary\n",
    "        })\n",
    "    \n",
    "    # 2) DA_AGONIST_YES1NOT0: n (x%)\n",
    "    if 'DA_AGONIST_YES1NOT0' in df.columns:\n",
    "        n_yes = sum(group_data['DA_AGONIST_YES1NOT0'] == 1)\n",
    "        pct_yes = 0 if total == 0 else round((n_yes / total) * 100, 1)\n",
    "        da_summary = f\"{n_yes} ({pct_yes}%)\"\n",
    "        cat_results.append({\n",
    "            \"Group\": grp,\n",
    "            \"Variable\": \"DA_AGONIST_YES1NOT0\",\n",
    "            \"Summary\": da_summary\n",
    "        })\n",
    "    \n",
    "    # 3) BDZ_YES1NOT0: n (x%)\n",
    "    if 'BDZ_YES1NOT0' in df.columns:\n",
    "        n_yes = sum(group_data['BDZ_YES1NOT0'] == 1)\n",
    "        pct_yes = 0 if total == 0 else round((n_yes / total) * 100, 1)\n",
    "        bdz_summary = f\"{n_yes} ({pct_yes}%)\"\n",
    "        cat_results.append({\n",
    "            \"Group\": grp,\n",
    "            \"Variable\": \"BDZ_YES1NOT0\",\n",
    "            \"Summary\": bdz_summary\n",
    "        })\n",
    "    \n",
    "    # 4) Hoehn & Yahr: solo stadi 1, 2, 3\n",
    "    if 'Hoen_Year' in df.columns:\n",
    "        for stage in [1, 2, 3]:\n",
    "            n_stage = sum(group_data['Hoen_Year'] == stage)\n",
    "            pct_stage = 0 if total == 0 else round((n_stage / total) * 100, 1)\n",
    "            hy_summary = f\"{n_stage} ({pct_stage}%)\"\n",
    "            cat_results.append({\n",
    "                \"Group\": grp,\n",
    "                \"Variable\": f\"Hoehn & Yahr = {stage}\",\n",
    "                \"Summary\": hy_summary\n",
    "            })\n",
    "\n",
    "categorical_summary_df = pd.DataFrame(cat_results)"
   ],
   "id": "2b410946f5050297",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T12:12:30.743115Z",
     "start_time": "2025-03-10T12:12:30.647377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# PARTE 3: TEST STATISTICI PER LE VARIABILI CATEGORICHE\n",
    "# ----------------------------\n",
    "\n",
    "cat_test_results = []\n",
    "cat_test_vars = ['Gender', 'DA_AGONIST_YES1NOT0', 'BDZ_YES1NOT0', 'Hoen_Year']\n",
    "\n",
    "for var in cat_test_vars:\n",
    "    if var not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    # Creiamo una copia temporanea del dataframe per le operazioni sulla variabile corrente\n",
    "    # In questo modo evitiamo di modificare il dataframe originale\n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    # Converto la variabile in stringa per evitare problemi di confronto\n",
    "    temp_df[var] = temp_df[var].astype(str)\n",
    "    \n",
    "    # Creo una tabella di contingenza\n",
    "    try:\n",
    "        contingency = pd.crosstab(temp_df['Group'], temp_df[var])\n",
    "        \n",
    "        # Filtro le righe per i gruppi in desired_order se esistono\n",
    "        existing_rows = [g for g in desired_order if g in contingency.index]\n",
    "        if existing_rows:\n",
    "            contingency = contingency.loc[existing_rows]\n",
    "            \n",
    "        # Non ordino le colonne per evitare errori di confronto\n",
    "        # Verifico se è possibile eseguire il test Chi-square\n",
    "        if contingency.shape[0] > 1 and contingency.shape[1] > 1:\n",
    "            # Solo per sicurezza, verifico che la tabella non abbia dimensione zero\n",
    "            if contingency.size > 0:\n",
    "                try:\n",
    "                    chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n",
    "                    \n",
    "                    # Verifico se ci sono celle con valori attesi piccoli\n",
    "                    expected_min = expected.min()\n",
    "                    cells_under_5 = (expected < 5).sum()\n",
    "                    pct_under_5 = cells_under_5 / expected.size * 100\n",
    "                    \n",
    "                    test_note = \"\"\n",
    "                    if pct_under_5 > 20 or expected_min < 1:\n",
    "                        test_name = \"Chi-square (warning: low expected counts)\"\n",
    "                        test_note = f\"{pct_under_5:.1f}% cells with expected count < 5\"\n",
    "                    else:\n",
    "                        test_name = \"Chi-square\"\n",
    "                    \n",
    "                    test_result = {\n",
    "                        \"Variable\": var,\n",
    "                        \"Test Used\": test_name,\n",
    "                        \"Statistic\": f\"chi2={chi2:.3f}, dof={dof}\",\n",
    "                        \"p-value\": round(p_val, 3),\n",
    "                        \"Note\": test_note\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    test_result = {\n",
    "                        \"Variable\": var,\n",
    "                        \"Test Used\": \"Error\",\n",
    "                        \"Statistic\": \"Error\",\n",
    "                        \"p-value\": \"NA\",\n",
    "                        \"Note\": f\"Error in chi-square: {str(e)}\"\n",
    "                    }\n",
    "            else:\n",
    "                test_result = {\n",
    "                    \"Variable\": var,\n",
    "                    \"Test Used\": \"Error\",\n",
    "                    \"Statistic\": \"Error\",\n",
    "                    \"p-value\": \"NA\",\n",
    "                    \"Note\": \"Empty contingency table\"\n",
    "                }\n",
    "        else:\n",
    "            test_result = {\n",
    "                \"Variable\": var,\n",
    "                \"Test Used\": \"Error\",\n",
    "                \"Statistic\": \"Error\",\n",
    "                \"p-value\": \"NA\",\n",
    "                \"Note\": \"Insufficient categories in contingency table\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        test_result = {\n",
    "            \"Variable\": var,\n",
    "            \"Test Used\": \"Error\",\n",
    "            \"Statistic\": \"Error\",\n",
    "            \"p-value\": \"NA\",\n",
    "            \"Note\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    cat_test_results.append(test_result)\n",
    "\n",
    "# Crea il dataframe dei risultati del test Chi-quadrato\n",
    "chi_square_df = pd.DataFrame(cat_test_results)\n",
    "\n",
    "# Salvataggio dei risultati su file Excel\n",
    "output_file = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/analysis_results.xlsx\"\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    numeric_summary_df.to_excel(writer, sheet_name='Numerical Variables Analysis', index=False)\n",
    "    categorical_summary_df.to_excel(writer, sheet_name='Categorical Variables Summary', index=False)\n",
    "    chi_square_df.to_excel(writer, sheet_name='Chi-Square Tests', index=False)\n",
    "\n",
    "print(f\"Analysis results saved to: {output_file}\")"
   ],
   "id": "ae675dcd87c31d45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis results saved to: C:\\Users\\Lorenzo\\Desktop\\scoring/analysis_results.xlsx\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T10:20:37.980726Z",
     "start_time": "2025-03-11T10:20:37.911436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import scikit_posthocs as sp  # per il Dunn test\n",
    "\n",
    "# ============================\n",
    "# FUNZIONI AUSILIARIE\n",
    "# ============================\n",
    "\n",
    "def pairwise_ttests_welch(data, groups, var, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Esegue confronti a coppie con t-test di Welch, applicando la correzione Bonferroni.\n",
    "    Restituisce una stringa con le comparazioni significative, usando notazione scientifica per i p-value.\n",
    "    (Non usata se si utilizza esclusivamente Tukey HSD per dati omogenei.)\n",
    "    \"\"\"\n",
    "    comparisons = []\n",
    "    pvals = []\n",
    "    group_list = list(groups)\n",
    "    for i in range(len(group_list)):\n",
    "        for j in range(i+1, len(group_list)):\n",
    "            grp1 = group_list[i]\n",
    "            grp2 = group_list[j]\n",
    "            d1 = data[data['Group'] == grp1][var].dropna()\n",
    "            d2 = data[data['Group'] == grp2][var].dropna()\n",
    "            if len(d1) < 2 or len(d2) < 2:\n",
    "                p = np.nan\n",
    "            else:\n",
    "                _, p = stats.ttest_ind(d1, d2, equal_var=False)\n",
    "            comparisons.append(f\"{grp1} vs {grp2}\")\n",
    "            pvals.append(p)\n",
    "    comparisons = np.array(comparisons)\n",
    "    pvals = np.array(pvals, dtype=float)\n",
    "    if len(pvals) > 0:\n",
    "        pvals_corr = np.minimum(pvals * len(pvals), 1.0)\n",
    "    else:\n",
    "        pvals_corr = pvals\n",
    "    significant = []\n",
    "    for comp, pcorr in zip(comparisons, pvals_corr):\n",
    "        if not np.isnan(pcorr) and pcorr < alpha:\n",
    "            p_str = np.format_float_scientific(pcorr, precision=3)\n",
    "            significant.append(f\"{comp} (p={p_str})\")\n",
    "    return \"; \".join(significant) if significant else \"None\"\n",
    "\n",
    "def pairwise_dunn(data, var, groups_used, desired_order, group_col='Group', alpha=0.05):\n",
    "    \"\"\"\n",
    "    Esegue il Dunn test con correzione Bonferroni integrata usando sp.posthoc_dunn.\n",
    "    Restituisce le comparazioni significative, formattate in notazione scientifica.\n",
    "    \"\"\"\n",
    "    sub_df = data[data[group_col].isin(groups_used)][[var, group_col]].dropna()\n",
    "    if len(sub_df) < 2 or len(groups_used) < 2:\n",
    "        return \"Insufficient data\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"scikit_posthocs\")\n",
    "        try:\n",
    "            dunn_res = sp.posthoc_dunn(sub_df, val_col=var, group_col=group_col, p_adjust='bonferroni')\n",
    "        except Exception as e:\n",
    "            return f\"Error in Dunn test: {e}\"\n",
    "    sig = []\n",
    "    groups_sorted = sorted(groups_used, key=lambda x: desired_order.index(x) if x in desired_order else len(desired_order))\n",
    "    for i in range(len(groups_sorted)):\n",
    "        for j in range(i+1, len(groups_sorted)):\n",
    "            p_val = dunn_res.loc[groups_sorted[i], groups_sorted[j]]\n",
    "            if p_val < alpha:\n",
    "                p_str = np.format_float_scientific(p_val, precision=3)\n",
    "                sig.append(f\"{groups_sorted[i]} vs {groups_sorted[j]} (p={p_str})\")\n",
    "    return \"; \".join(sig) if sig else \"None\"\n",
    "\n",
    "def pairwise_dunn_bonferroni(data, var, groups_used, desired_order, group_col='Group', alpha=0.05):\n",
    "    \"\"\"\n",
    "    Esegue il Dunn test senza correzione e poi applica manualmente la correzione Bonferroni.\n",
    "    Restituisce le comparazioni significative, con i p-value in notazione scientifica.\n",
    "    \"\"\"\n",
    "    sub_df = data[data[group_col].isin(groups_used)][[var, group_col]].dropna()\n",
    "    if len(sub_df) < 2 or len(groups_used) < 2:\n",
    "        return \"Insufficient data\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"scikit_posthocs\")\n",
    "        try:\n",
    "            dunn_res = sp.posthoc_dunn(sub_df, val_col=var, group_col=group_col, p_adjust=None)\n",
    "        except Exception as e:\n",
    "            return f\"Error in Dunn test: {e}\"\n",
    "    n = len(groups_used)\n",
    "    n_comparisons = n * (n - 1) / 2\n",
    "    bonf_pvals = dunn_res * n_comparisons\n",
    "    bonf_pvals = bonf_pvals.clip(upper=1.0)\n",
    "    sig = []\n",
    "    groups_sorted = sorted(groups_used, key=lambda x: desired_order.index(x) if x in desired_order else len(desired_order))\n",
    "    for i in range(len(groups_sorted)):\n",
    "        for j in range(i+1, len(groups_sorted)):\n",
    "            p_val = bonf_pvals.loc[groups_sorted[i], groups_sorted[j]]\n",
    "            if p_val < alpha:\n",
    "                p_str = np.format_float_scientific(p_val, precision=3)\n",
    "                sig.append(f\"{groups_sorted[i]} vs {groups_sorted[j]} (p={p_str})\")\n",
    "    return \"; \".join(sig) if sig else \"None\"\n",
    "\n",
    "def welch_anova(samples, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Calcola Welch's ANOVA per una lista di array (samples).\n",
    "    Se un gruppo ha varianza zero, viene sostituita con 'epsilon'.\n",
    "    Ritorna F e p-value.\n",
    "    \"\"\"\n",
    "    k = len(samples)\n",
    "    if k < 2 or any(len(s) < 2 for s in samples):\n",
    "        return np.nan, np.nan\n",
    "    ns = np.array([len(s) for s in samples])\n",
    "    means = np.array([np.mean(s) for s in samples])\n",
    "    variances = np.array([np.var(s, ddof=1) for s in samples])\n",
    "    variances = np.where(variances == 0, epsilon, variances)\n",
    "    weights = ns / variances\n",
    "    grand_mean = np.sum(weights * means) / np.sum(weights)\n",
    "    numerator = np.sum(weights * (means - grand_mean)**2)\n",
    "    F = numerator / (k - 1)\n",
    "    denominator_terms = (1 - weights/np.sum(weights))**2 / (ns - 1)\n",
    "    if np.sum(denominator_terms) == 0:\n",
    "        return np.nan, np.nan\n",
    "    df_den = (k**2 - 1) / (3 * np.sum(denominator_terms))\n",
    "    if not np.isfinite(F) or not np.isfinite(df_den) or df_den <= 0:\n",
    "        return np.nan, np.nan\n",
    "    p_val = 1 - stats.f.cdf(F, k - 1, df_den)\n",
    "    return F, p_val\n",
    "\n",
    "def group_descriptive_stats_dict(data, groups, var, norm_status):\n",
    "    \"\"\"\n",
    "    Calcola le statistiche descrittive per la variabile 'var' per ogni gruppo.\n",
    "    Se i dati sono normali: restituisce \"mean ± sd\".\n",
    "    Se i dati NON sono normali: restituisce \"median (IQR)\", dove IQR = Q3 - Q1.\n",
    "    Se il gruppo è costante (std == 0) o mancano dati, restituisce \"-\".\n",
    "    Ritorna un dizionario { gruppo: stringa }.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for grp in groups:\n",
    "        vals = data[data['Group'] == grp][var].dropna()\n",
    "        if len(vals) == 0 or np.std(vals, ddof=1) == 0:\n",
    "            d[grp] = \"-\"\n",
    "        else:\n",
    "            if norm_status == \"Normal\":\n",
    "                d[grp] = f\"{np.round(np.mean(vals),2)} ± {np.round(np.std(vals, ddof=1),2)}\"\n",
    "            else:\n",
    "                median_val = np.median(vals)\n",
    "                q1 = np.percentile(vals, 25)\n",
    "                q3 = np.percentile(vals, 75)\n",
    "                iqr = q3 - q1\n",
    "                d[grp] = f\"{np.round(median_val,2)} ({np.round(iqr,2)})\"\n",
    "    return d"
   ],
   "id": "af8d4afbf42d5c4f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T12:35:03.291955Z",
     "start_time": "2025-03-10T12:35:03.135462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Caricamento dei dati\n",
    "# ============================\n",
    "\n",
    "data_dir = r\"C:\\Users\\Lorenzo\\Desktop\\scoring\\demographics\"\n",
    "data_file = os.path.join(data_dir, \"summary_extracted.xlsx\")\n",
    "nv_file = os.path.join(data_dir, \"normality_variance_summary.xlsx\")\n",
    "output_file = os.path.join(data_dir, \"final_analysis.xlsx\")\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"ERRORE: File dati non trovato: {data_file}\")\n",
    "    exit(1)\n",
    "if not os.path.exists(nv_file):\n",
    "    print(f\"ERRORE: File normalità/varianza non trovato: {nv_file}\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(data_file)\n",
    "    df['Group'] = df['Group'].astype('category')\n",
    "    nv_df = pd.read_excel(nv_file)\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE nel caricamento dei file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "numeric_vars = ['Age', 'Disease_Duration', 'LEDD', 'UPDRSIV4.1', 'UPDRSIV4.2', 'UPDRSIV4.3']\n",
    "numeric_vars = [var for var in numeric_vars if var in df.columns]\n",
    "\n",
    "# ============================\n",
    "# Statistiche descrittive\n",
    "# ============================\n",
    "\n",
    "descriptive_stats = {}\n",
    "for var in numeric_vars:\n",
    "    norm_status = nv_df.loc[nv_df['Variable'] == var, 'Normality'].values[0]\n",
    "    descriptive_stats[var] = group_descriptive_stats_dict(df, df['Group'].unique(), var, norm_status)\n",
    "    \n",
    "# ============================\n",
    "# Test Statistici\n",
    "# ============================\n",
    "\n",
    "test_results = {}\n",
    "for var in numeric_vars:\n",
    "    norm_status = nv_df.loc[nv_df['Variable'] == var, 'Normality'].values[0]\n",
    "    if norm_status == \"Normal\":\n",
    "        result = stats.f_oneway(*(df[df['Group'] == grp][var].dropna() for grp in df['Group'].unique()))\n",
    "        test_results[var] = result\n",
    "    else:\n",
    "        result = stats.kruskal(*(df[df['Group'] == grp][var].dropna() for grp in df['Group'].unique()))\n",
    "        test_results[var] = result"
   ],
   "id": "f942f531981ef7d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lorenzo\\AppData\\Local\\Temp\\ipykernel_9316\\3910445438.py:45: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  result = stats.f_oneway(*(df[df['Group'] == grp][var].dropna() for grp in df['Group'].unique()))\n",
      "C:\\Users\\Lorenzo\\AppData\\Local\\Temp\\ipykernel_9316\\3910445438.py:48: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  result = stats.kruskal(*(df[df['Group'] == grp][var].dropna() for grp in df['Group'].unique()))\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T12:42:25.328598Z",
     "start_time": "2025-03-10T12:42:23.137803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Percorsi ai file\n",
    "demographics_path = r\"C:\\Users\\Lorenzo\\Desktop\\scoring\\demographics/summary_extracted.xlsx\"\n",
    "sleep_path = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/sleep_characteristics.xlsx\"\n",
    "\n",
    "# Cartella di destinazione per i grafici di correlazione\n",
    "correlation_folder = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\"\n",
    "os.makedirs(correlation_folder, exist_ok=True)\n",
    "\n",
    "# Carica i dati\n",
    "df_dem = pd.read_excel(demographics_path)\n",
    "df_sleep = pd.read_excel(sleep_path)\n",
    "\n",
    "# Unione dei DataFrame: usa \"ID\" per i demografici e \"Patient\" per lo scoring\n",
    "df_merged = pd.merge(df_dem, df_sleep, left_on=\"ID\", right_on=\"Patient\")\n",
    "\n",
    "# Escludi i controlli (gruppo \"CTL\")\n",
    "df_merged = df_merged[df_merged[\"Group_x\"] != \"CTL\"]\n",
    "\n",
    "# Calcolo della correlazione di Pearson utilizzando solo i dati dei pazienti (non controlli)\n",
    "r, p_value = pearsonr(df_merged[\"Disease_Duration\"], df_merged[\"Total Sleep Time (min)\"])\n",
    "print(\"Correlazione Pearson: r = {:.3f}, p-value = {:.3f}\".format(r, p_value))\n",
    "\n",
    "# Creazione del grafico di dispersione con linea di regressione\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=\"Disease_Duration\", y=\"Total Sleep Time (min)\", data=df_merged, ci=95)\n",
    "plt.title(\"All Patients\")\n",
    "plt.xlabel(\"Disease Duration\")\n",
    "plt.ylabel(\"Total Sleep Time (min)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Aggiungi l'annotazione dei risultati di Pearson nell'angolo in alto a destra\n",
    "plt.text(0.95, 0.95, f\"r = {r:.3f}\\\\np = {p_value:.3f}\", \n",
    "         transform=plt.gca().transAxes, \n",
    "         horizontalalignment='right', \n",
    "         verticalalignment='top', \n",
    "         fontsize=12,\n",
    "         )\n",
    "\n",
    "# Salvataggio del grafico nella cartella \"correlation\"\n",
    "output_path = os.path.join(correlation_folder, \"disease_duration_vs_tst_no_CTL.png\")\n",
    "plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Grafico salvato in: {output_path}\")\n",
    "\n",
    "# Lista dei gruppi da analizzare\n",
    "groups = [\"DNV\", \"ADV\", \"DYS\"]\n",
    "\n",
    "for group in groups:\n",
    "    # Filtra il DataFrame per il gruppo corrente\n",
    "    df_group = df_merged[df_merged[\"Group_x\"] == group]\n",
    "    \n",
    "    # Calcola la correlazione di Pearson per il gruppo corrente\n",
    "    r, p_value = pearsonr(df_group[\"Disease_Duration\"], df_group[\"Total Sleep Time (min)\"])\n",
    "    print(f\"Gruppo {group} - Correlazione Pearson: r = {r:.3f}, p-value = {p_value:.3f}\")\n",
    "    \n",
    "    # Crea il grafico di dispersione con linea di regressione\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=\"Disease_Duration\", y=\"Total Sleep Time (min)\", data=df_group, ci=95)\n",
    "    plt.title(f\"{group}\")\n",
    "    plt.xlabel(\"Disease Duration\")\n",
    "    plt.ylabel(\"Total Sleep Time (min)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # Aggiungi l'annotazione dei risultati di Pearson in alto a destra\n",
    "    plt.text(0.95, 0.95, f\"r = {r:.3f}\\\\np = {p_value:.3f}\", \n",
    "             transform=plt.gca().transAxes, \n",
    "             horizontalalignment='right', \n",
    "             verticalalignment='top', \n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    \n",
    "    # Salva il grafico per il gruppo corrente\n",
    "    output_path = os.path.join(correlation_folder, f\"disease_duration_vs_tst_{group}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Grafico salvato in: {output_path}\")\n"
   ],
   "id": "c29ecf204d2ff04b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlazione Pearson: r = -0.472, p-value = 0.023\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_tst_no_CTL.png\n",
      "Gruppo DNV - Correlazione Pearson: r = 0.730, p-value = 0.161\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_tst_DNV.png\n",
      "Gruppo ADV - Correlazione Pearson: r = -0.696, p-value = 0.055\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_tst_ADV.png\n",
      "Gruppo DYS - Correlazione Pearson: r = -0.324, p-value = 0.362\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_tst_DYS.png\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T11:33:08.169199Z",
     "start_time": "2025-03-08T11:33:00.556013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Percorsi ai file\n",
    "demographics_path = r\"C:\\Users\\Lorenzo\\Desktop\\scoring\\demographics/summary_extracted.xlsx\"\n",
    "sleep_path = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/sleep_characteristics.xlsx\"\n",
    "\n",
    "# Cartella di destinazione per i grafici di correlazione\n",
    "correlation_folder = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\"\n",
    "os.makedirs(correlation_folder, exist_ok=True)\n",
    "\n",
    "# Carica i dati\n",
    "df_dem = pd.read_excel(demographics_path)\n",
    "df_sleep = pd.read_excel(sleep_path)\n",
    "\n",
    "# Unione dei DataFrame: \"ID\" per i demografici e \"Patient\" per lo scoring\n",
    "df_merged = pd.merge(df_dem, df_sleep, left_on=\"ID\", right_on=\"Patient\")\n",
    "\n",
    "# Escludi i controlli (gruppo \"CTL\")\n",
    "df_merged = df_merged[df_merged[\"Group_x\"] != \"CTL\"]\n",
    "\n",
    "# ANALISI: Sleep Efficiency (%) vs Disease Duration\n",
    "\n",
    "# 1. Analisi per tutti i pazienti (escludendo CTL) utilizzando Spearman\n",
    "r_all, p_all = spearmanr(df_merged[\"Disease_Duration\"], df_merged[\"Sleep Efficiency (%)\"])\n",
    "print(f\"All patients - Spearman correlation: r = {r_all:.3f}, p = {p_all:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=\"Disease_Duration\", y=\"Sleep Efficiency (%)\", data=df_merged, ci=95)\n",
    "plt.title(\"All Patients (non-CTL)\")\n",
    "plt.xlabel(\"Disease Duration\")\n",
    "plt.ylabel(\"Sleep Efficiency (%)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.text(0.95, 0.95, f\"r = {r_all:.3f}\\\\np = {p_all:.3f}\", \n",
    "         transform=plt.gca().transAxes, \n",
    "         horizontalalignment='right', \n",
    "         verticalalignment='top', \n",
    "         fontsize=12,\n",
    "         bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "output_path_all = os.path.join(correlation_folder, \"disease_duration_vs_sleep_efficiency_no_CTL_all.png\")\n",
    "plt.savefig(output_path_all, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"Grafico salvato in: {output_path_all}\")\n",
    "\n",
    "# 2. Analisi per ciascun gruppo: DNV, ADV, DYS\n",
    "groups = [\"DNV\", \"ADV\", \"DYS\"]\n",
    "\n",
    "for group in groups:\n",
    "    df_group = df_merged[df_merged[\"Group_x\"] == group]\n",
    "    r_group, p_group = spearmanr(df_group[\"Disease_Duration\"], df_group[\"Sleep Efficiency (%)\"])\n",
    "    print(f\"Gruppo {group} - Spearman correlation: r = {r_group:.3f}, p = {p_group:.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=\"Disease_Duration\", y=\"Sleep Efficiency (%)\", data=df_group, ci=95)\n",
    "    plt.title(f\"Group: {group}\")\n",
    "    plt.xlabel(\"Disease Duration\")\n",
    "    plt.ylabel(\"Sleep Efficiency (%)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.text(0.95, 0.95, f\"r = {r_group:.3f}\\\\np = {p_group:.3f}\", \n",
    "             transform=plt.gca().transAxes, \n",
    "             horizontalalignment='right', \n",
    "             verticalalignment='top', \n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    output_path_group = os.path.join(correlation_folder, f\"disease_duration_vs_sleep_efficiency_{group}.png\")\n",
    "    plt.savefig(output_path_group, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Grafico salvato in: {output_path_group}\")\n",
    "\n",
    "# Nuovo blocco di codice\n",
    "\n",
    "# Percorsi ai file\n",
    "demographics_path = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/demographics/summary_extracted.xlsx\"\n",
    "sleep_path = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/sleep_characteristics.xlsx\"\n",
    "\n",
    "# Cartella di destinazione per i grafici di correlazione\n",
    "correlation_folder = r\"C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\"\n",
    "os.makedirs(correlation_folder, exist_ok=True)\n",
    "\n",
    "# Carica i dati\n",
    "df_dem = pd.read_excel(demographics_path)\n",
    "df_sleep = pd.read_excel(sleep_path)\n",
    "\n",
    "# Unione dei DataFrame: \"ID\" per i demografici e \"Patient\" per lo scoring\n",
    "df_merged = pd.merge(df_dem, df_sleep, left_on=\"ID\", right_on=\"Patient\")\n",
    "\n",
    "# Consideriamo solo i gruppi di interesse: DNV, ADV, DYS (utilizzando la colonna \"Group_x\" dei dati demografici)\n",
    "groups = [\"DNV\", \"ADV\", \"DYS\"]\n",
    "df_filtered = df_merged[df_merged[\"Group_x\"].isin(groups)]\n",
    "\n",
    "# Itera sui gruppi per calcolare la correlazione tra LEDD e REM% \n",
    "for group in groups:\n",
    "    df_group = df_filtered[df_filtered[\"Group_x\"] == group].dropna(subset=[\"LEDD\", \"REM%\"])\n",
    "    \n",
    "    if len(df_group) < 3:\n",
    "        print(f\"Gruppo {group} ha meno di 3 soggetti validi, salto il calcolo.\")\n",
    "        continue\n",
    "    \n",
    "    r, p_value = spearmanr(df_group[\"LEDD\"], df_group[\"REM%\"])\n",
    "    print(f\"Gruppo {group} - Spearman correlation: r = {r:.3f}, p-value = {p_value:.3f}, n = {len(df_group)}\")\n",
    "    \n",
    "    # Creazione del grafico di dispersione con linea di regressione\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=\"LEDD\", y=\"REM%\", data=df_group, ci=95)\n",
    "    plt.title(f\"Correlazione LEDD vs REM% in gruppo {group}\")\n",
    "    plt.xlabel(\"LEDD\")\n",
    "    plt.ylabel(\"REM (%)\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # Annotazione dei risultati di Spearman in alto a destra\n",
    "    plt.text(0.95, 0.95, f\"r = {r:.3f}\\\\np = {p_value:.3f}\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             horizontalalignment=\"right\",\n",
    "             verticalalignment=\"top\",\n",
    "             fontsize=12,\n",
    "             bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"none\"))\n",
    "    \n",
    "    output_path = os.path.join(correlation_folder, f\"LEDD_vs_REMperc_{group}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Grafico salvato in: {output_path}\")\n"
   ],
   "id": "c0869acc378f7a36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All patients - Spearman correlation: r = -0.360, p = 0.092\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_sleep_efficiency_no_CTL_all.png\n",
      "Gruppo DNV - Spearman correlation: r = 0.100, p = 0.873\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_sleep_efficiency_DNV.png\n",
      "Gruppo ADV - Spearman correlation: r = -0.119, p = 0.779\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_sleep_efficiency_ADV.png\n",
      "Gruppo DYS - Spearman correlation: r = -0.158, p = 0.663\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\disease_duration_vs_sleep_efficiency_DYS.png\n",
      "Gruppo DNV - Spearman correlation: r = 0.000, p-value = 1.000, n = 5\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\LEDD_vs_REMperc_DNV.png\n",
      "Gruppo ADV - Spearman correlation: r = 0.494, p-value = 0.213, n = 8\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\LEDD_vs_REMperc_ADV.png\n",
      "Gruppo DYS - Spearman correlation: r = -0.515, p-value = 0.128, n = 10\n",
      "Grafico salvato in: C:\\Users\\Lorenzo\\Desktop\\scoring/correlation\\LEDD_vs_REMperc_DYS.png\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T11:33:24.644269Z",
     "start_time": "2025-03-08T11:33:20.786130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Carica i dataset\n",
    "df_dem = pd.read_excel(r\"C:\\Users\\Lorenzo\\Desktop\\scoring/demographics/summary_extracted.xlsx\")\n",
    "df_sleep = pd.read_excel(r\"C:\\Users\\Lorenzo\\Desktop\\scoring/sleep_characteristics.xlsx\")\n",
    "\n",
    "# Unisci i dataset usando \"ID\" e \"Patient\"\n",
    "df = pd.merge(df_dem, df_sleep, left_on=\"ID\", right_on=\"Patient\")\n",
    "\n",
    "# Creazione della variabile binaria per DYS\n",
    "# Supponiamo che in 'Group' del file demografico (Group_x) il valore 'DYS' indichi la presenza di discinesie\n",
    "df['DYS_flag'] = df['Group_x'].apply(lambda x: 1 if x == 'DYS' else 0)\n",
    "\n",
    "# Per comodità, rinominiamo alcune colonne per evitare spazi e caratteri speciali\n",
    "df = df.rename(columns={\n",
    "    \"Sleep Efficiency (%)\": \"SleepEff\",\n",
    "    \"REM%\": \"REM_perc\",\n",
    "    \"Disease_Duration\": \"DiseaseDuration\"\n",
    "})\n",
    "\n",
    "# Esempio di regressione logistica univariata per LEDD\n",
    "model_ledd = smf.logit(\"DYS_flag ~ LEDD\", data=df).fit(disp=False)\n",
    "print(model_ledd.summary())\n",
    "\n",
    "# Esegui regressioni univariate per le altre variabili di interesse (Age, DiseaseDuration, Gender, SleepEff, REM_perc)\n",
    "# Nota: per Gender, se è una stringa (es. \"Male\"/\"Female\"), conviene codificarla:\n",
    "df['Gender_num'] = df['Gender'].apply(lambda x: 1 if str(x).lower() == 'male' else 0)\n",
    "\n",
    "variables = [\"Age\", \"DiseaseDuration\", \"LEDD\", \"Gender_num\", \"SleepEff\", \"REM_perc\"]\n",
    "\n",
    "selected_vars = []\n",
    "for var in variables:\n",
    "    formula = f\"DYS_flag ~ {var}\"\n",
    "    model_uni = smf.logit(formula, data=df).fit(disp=False)\n",
    "    p_val = model_uni.pvalues[var]\n",
    "    print(f\"Variabile: {var} - p-value: {p_val:.3f}\")\n",
    "    if p_val < 0.2:\n",
    "        selected_vars.append(var)\n",
    "\n",
    "# Se la letteratura suggerisce l'inclusione di alcune variabili, assicurati che siano presenti\n",
    "# Ad esempio, se 'Age' non fosse selezionato ma è ritenuto importante:\n",
    "if \"Age\" not in selected_vars:\n",
    "    selected_vars.append(\"Age\")\n",
    "\n",
    "print(\"Variabili selezionate per il modello multivariato:\", selected_vars)\n",
    "\n",
    "# Costruisci il modello di regressione logistica multivariata\n",
    "formula_multi = \"DYS_flag ~ \" + \" + \".join(selected_vars)\n",
    "model_multi = smf.logit(formula_multi, data=df).fit(disp=False)\n",
    "print(model_multi.summary())\n",
    "\n",
    "# Calcola Odds Ratios e 95% CI\n",
    "params = model_multi.params\n",
    "conf = model_multi.conf_int()\n",
    "OR = pd.DataFrame({\n",
    "    \"OR\": np.exp(params),\n",
    "    \"2.5%\": np.exp(conf[0]),\n",
    "    \"97.5%\": np.exp(conf[1])\n",
    "})\n",
    "print(OR)\n"
   ],
   "id": "181103bcbc81d793",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               DYS_flag   No. Observations:                   23\n",
      "Model:                          Logit   Df Residuals:                       21\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Sat, 08 Mar 2025   Pseudo R-squ.:                  0.4373\n",
      "Time:                        12:33:20   Log-Likelihood:                -8.8611\n",
      "converged:                       True   LL-Null:                       -15.746\n",
      "Covariance Type:            nonrobust   LLR p-value:                 0.0002066\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -3.7779      1.559     -2.423      0.015      -6.834      -0.722\n",
      "LEDD           0.0063      0.003      2.438      0.015       0.001       0.011\n",
      "==============================================================================\n",
      "Variabile: Age - p-value: 0.260\n",
      "Variabile: DiseaseDuration - p-value: 0.024\n",
      "Variabile: LEDD - p-value: 0.015\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m var \u001B[38;5;129;01min\u001B[39;00m variables:\n\u001B[0;32m     35\u001B[0m     formula \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDYS_flag ~ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvar\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 36\u001B[0m     model_uni \u001B[38;5;241m=\u001B[39m \u001B[43msmf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformula\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdisp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     p_val \u001B[38;5;241m=\u001B[39m model_uni\u001B[38;5;241m.\u001B[39mpvalues[var]\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVariabile: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvar\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - p-value: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp_val\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pycharmturorial\\.venv\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2601\u001B[0m, in \u001B[0;36mLogit.fit\u001B[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001B[0m\n\u001B[0;32m   2598\u001B[0m \u001B[38;5;129m@Appender\u001B[39m(DiscreteModel\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__doc__\u001B[39m)\n\u001B[0;32m   2599\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, start_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewton\u001B[39m\u001B[38;5;124m'\u001B[39m, maxiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m35\u001B[39m,\n\u001B[0;32m   2600\u001B[0m         full_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, disp\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m-> 2601\u001B[0m     bnryfit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2602\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2603\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mmaxiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaxiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2604\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mfull_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfull_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2605\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mdisp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2606\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2607\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2609\u001B[0m     discretefit \u001B[38;5;241m=\u001B[39m LogitResults(\u001B[38;5;28mself\u001B[39m, bnryfit)\n\u001B[0;32m   2610\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pycharmturorial\\.venv\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:243\u001B[0m, in \u001B[0;36mDiscreteModel.fit\u001B[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# TODO: make a function factory to have multiple call-backs\u001B[39;00m\n\u001B[1;32m--> 243\u001B[0m mlefit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    245\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mmaxiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaxiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    246\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mfull_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfull_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mdisp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[43m                     \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mlefit\n",
      "File \u001B[1;32m~\\PycharmProjects\\pycharmturorial\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:582\u001B[0m, in \u001B[0;36mLikelihoodModel.fit\u001B[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001B[0m\n\u001B[0;32m    580\u001B[0m     Hinv \u001B[38;5;241m=\u001B[39m cov_params_func(\u001B[38;5;28mself\u001B[39m, xopt, retvals)\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewton\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m full_output:\n\u001B[1;32m--> 582\u001B[0m     Hinv \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mretvals\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mHessian\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m nobs\n\u001B[0;32m    583\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m skip_hessian:\n\u001B[0;32m    584\u001B[0m     H \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhessian(xopt)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pycharmturorial\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:609\u001B[0m, in \u001B[0;36minv\u001B[1;34m(a)\u001B[0m\n\u001B[0;32m    606\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD->D\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md->d\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m errstate(call\u001B[38;5;241m=\u001B[39m_raise_linalgerror_singular, invalid\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcall\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    608\u001B[0m               over\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m, divide\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m, under\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 609\u001B[0m     ainv \u001B[38;5;241m=\u001B[39m \u001B[43m_umath_linalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minv\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap(ainv\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\pycharmturorial\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:104\u001B[0m, in \u001B[0;36m_raise_linalgerror_singular\u001B[1;34m(err, flag)\u001B[0m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_raise_linalgerror_singular\u001B[39m(err, flag):\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSingular matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mLinAlgError\u001B[0m: Singular matrix"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
